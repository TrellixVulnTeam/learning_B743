# torchtext

1. link
   * `pytorch/Tutorials/Text` [link](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)
   * transformer, Attention is all you need [link](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
   * BERT [arxiv-link](https://arxiv.org/abs/1810.04805)
2. install
   * `conda install -c pytorch torchtext`
   * `conda install -c conda-forge spacy click=7`
3. download spacy tokenizer
   * `python -m spacy download en_core_web_sm`
   * `python -m spacy download en_core_web_md`
   * `python -m spacy download de_core_news_sm`
4. PPL: Perplexity

## seq2seq

1. link
   * Sequence to Sequence Learning with Neural Networks [arxiv-link](https://arxiv.org/abs/1409.3215)
   * Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation [arxiv-link](https://arxiv.org/abs/1406.1078)
2. attention
   * Neural Machine Translation by Jointly Learning to Align and Translate [arxiv-link](https://arxiv.org/abs/1409.0473)
   * Effective Approaches to Attention-based Neural Machine Translation [arxiv-link](https://arxiv.org/abs/1508.04025)

## transformer

1. link
   * [harvardnlp/the-annotated-transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
   * [blog/JianlinSu](https://kexue.fm/)
   * [Ja-Alammar/the-illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)
   * [arxiv](https://arxiv.org/abs/1706.03762)
   * [github/external-attention-pytorch](https://github.com/xmu-xiaoma666/External-Attention-pytorch)
2. [github/opennmt/opennmt-py](https://github.com/opennmt/opennmt-py#pretrained-models)
3. BPE, beam search, subword-nmt

## bert

1. link
   * [arxiv](https://arxiv.org/abs/1810.04805)
